---
title: "dippelmax_HW4_AN588"
author: "Max Dippel"
date: "10/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().
When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().
The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
The function should contain a check for the rules of thumb we have talked about (n∗p>5 and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

```{r}
# data I need to test one tailed z test
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/vervet-weights.csv")
monkeys <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(monkeys)
```


```{r}
# One tailed z test
mean(monkeys$weight)
mu <- 5.0
x <- monkeys$weight
p <- 1 - pnorm((mean(x) - mu)/(sd(x)/sqrt(length(x))))
p

p1 <- .6
p0 <- .8
n <- 30
  
p <- pnorm((p1 - p0)/sqrt(pi * (1 - p0)/n))
```

```{r}
# Simple function for one tailed test
Z.prop.test <- function(p1, p0, n) {
    pnorm((p1 - p0)/sqrt(pi * (1 - p0)/n))
}
```


```{r}
# one or two tailed z-test function


Z.prop.test <- function(p1, p0, n1, p2=NULL, n2=NULL) { 
  if (is.null(p2)) {
    z <- (p1 - p0)/sqrt(pi * (1 - p0)/n1)
    p <- pnorm((p1 - p0)/sqrt(pi * (1 - p0)/n1))
    lower <- p1 - qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
  return (list(p.value = as.numeric(p), z.value = as.numeric(z), Lower.confidence.interval = as.numeric(lower), 
                 upper.confidence.interval = as.numeric(upper))) } 
  
  else {
    z <-  (p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2))
    p <- pnorm((p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2)))
    upper <- (p1 - p2) + (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
    lower <- (p1 - p2) - (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
    return (list(p.value = as.numeric(p), z.value = as.numeric(z), Lower.confidence.interval = as.numeric(lower), 
                 upper.confidence.interval = as.numeric(upper)))
  }
}


```



```{r}
# Adding more functions

Z.prop.test <- function(p1, p0, n1, p2=NULL, n2=NULL) { 
  if (is.null(p2)) {
    z <- (p1 - p0)/sqrt(pi * (1 - p0)/n1)
    p <- pnorm((p1 - p0)/sqrt(pi * (1 - p0)/n1))
    lower <- p1 - qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
  return (list(p.value = as.numeric(p), z.value = as.numeric(z), Lower.confidence.interval = as.numeric(lower), 
                 upper.confidence.interval = as.numeric(upper))) 
    if (n1 * p1 > 5 || n1 (1- p1) > 5) { print("Error")}
    } 
  
  else {
    z <-  (p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2))
    p <- pnorm((p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2)))
    upper <- (p1 - p2) + (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
    lower <- (p1 - p2) - (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
    return (list(p.value = as.numeric(p), z.value = as.numeric(z), Lower.confidence.interval = as.numeric(lower), 
                 upper.confidence.interval = as.numeric(upper)))
    
  }
}

```


```{r}
# Testing results

Z.prop.test(p1 = .6, p0 = .9, n1 = 30)

Z.prop.test(p1 = .9, n1 = 30, p2 = .4, n2 = 30)

p1 <- .6
p0 <- .8
n1 <- 30
p2 <- .3
n2 <- 30
z <- -2.08733

z <-  (p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2))
p <- pnorm((p2 - p1)/sqrt((p2 + p1/(n1 + n2) * (1 - (p1 + p2)/(n1 + n2))) * (1/n1 + 1/n2)))

upper <- (p1 - p2) + (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
lower <- (p1 - p2) - (z * sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
  
```

*This is good, basically improving the previous function by testing for the rule of thumb for the distribution. You do, however, want to assign "alternative" to "two.sided" as default, then you can also indicate whether you are looking at the lower tail, upper, or both (alternative = "less", "greater", or "two.sided", respectively). Then, it can also be tested using these parameteres. But otherwise it looks really good!*



```{r}
# data I need to test two tailed z test
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/colobus-weights.csv")
d2 <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d2)
x <- d2$weight[d2$sex == "male"]
y <- d2$weight[d2$sex == "female"]
```

```{r}
# two tailed z test 

```




[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):
Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).
Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.
Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
Looking at your two models, which do you think is better? Why?

```{r}
library(ggplot2)
# d <- read.csv("KamilarAndCooperData.csv", header = TRUE)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
d <- na.omit(d)

# Making a linear model

lm1 <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
summary(lm1)

# Finding confidence intervals for parameters b0 and b1

ci1 <- confint(lm1, level = 0.90)
ci1

# making confidence intervals for line 

ci3 <- predict(lm1, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence",
    level = 0.90)
head(ci3)
lines <- data.frame(ci3)

# Making prediction intervals for line

ci5 <- predict(lm1, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "prediction",
    level = 0.90)
head(ci5)
lines3 <- data.frame(ci5)

# Plotting the lines on a graph

text <- "(Longevity)= 0.8789(Mean brain size) + 307.7522"

ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) + geom_point(color="darkorange3") + 
  geom_text(aes(x = 250, y = 750, label=text)) + 
  geom_line(aes(x = d$Brain_Size_Species_Mean, y = lines$fit, color = "black")) +
  geom_line(aes(x = d$Brain_Size_Species_Mean, y = lines$lwr, color = "orange")) + 
  geom_line(aes(x = d$Brain_Size_Species_Mean, y = lines$upr, color = "purple")) + 
  geom_line(aes(x = d$Brain_Size_Species_Mean, y = lines3$lwr, color = "green")) + 
  geom_line(aes(x = d$Brain_Size_Species_Mean, y = lines3$upr, color = "blue")) + 
  scale_color_discrete(name = "legend", labels = c("Fit", "Upper Prediction Interval", "Lower Prediction Interval", "Lower Confidence Interval" , "Upper Confidence Interval"))

```

*beautiful*

```{r}

# Making log() linear model

lm2 <- lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean), data = d)
summary(lm2)

# Finding confidence intervals for parameters b0 and b1

ci2 <- confint(lm2, level = 0.90)
ci2

# Making confidence intervals for line

ci4 <- predict(lm1, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence",
    level = 0.90)
head(ci4)
lines2 <- data.frame(ci4)

# Making prediction intervals for line

ci6 <- predict(lm2, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "prediction",
    level = 0.90)
head(ci6)
lines4 <- data.frame(ci6)

# Plotting the lines on a graph

text2 <- "log(Longevity)= 0.20285*log(Mean brain size) + 5.08650"
ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m))) + geom_point(color="darkorange") + geom_text(aes(x = 4, y = 7, label=text2)) + 
geom_line(aes(x = log(d$Brain_Size_Species_Mean), y = log(lines2$fit), color = "black")) +
geom_line(aes(x = log(d$Brain_Size_Species_Mean), y = log(lines2$lwr), color = "orange")) + 
  geom_line(aes(x = log(d$Brain_Size_Species_Mean), y = log(lines2$upr), color = "red")) +
   geom_line(aes(x = log(d$Brain_Size_Species_Mean), y = lines4$lwr, color = "green")) + 
  geom_line(aes(x = log(d$Brain_Size_Species_Mean), y = lines4$upr, color = "blue")) + scale_color_discrete(name = "legend", labels = c("Fit", "Upper Prediction Interval", "Lower Prediction Interval", "Lower Confidence Interval" , "Upper Confidence Interval"))

# Confidence intervals for 800

ci7 <- predict(lm1, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction",
    level = 0.90)
ci8 <- predict(lm2, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction",
    level = 0.90)

# Yes I trust this estimate. It came from my model and seems accurate. 

# The log model looks like it has a better fit 

```

*taking the log of your variables will help you get a straight best fit line*
*re: Brain_Size_Species_Mean = 800: Just because it came from your model doesn't make it right! (although i love the confidence). Remember, models are wrong, but helpful. Finding the value for when Brain_Size_Species_Mean = 800 may not be helpful in this case because it is so far off from the data that we have put into our model.*
